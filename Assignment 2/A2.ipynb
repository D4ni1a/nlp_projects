{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "### by Danila Shulepin (BS21-DS-02)\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed."
      ],
      "metadata": {
        "id": "Nj-9DhVQpIcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation of context-sensitive spelling correction"
      ],
      "metadata": {
        "id": "HHivWX8EpmKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading and importing packages"
      ],
      "metadata": {
        "id": "7dFL7R8rxVtY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "_Tv9xsYqra2D"
      },
      "outputs": [],
      "source": [
        "# !pip install annoy\n",
        "# !pip install gdown\n",
        "# # !pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gdown\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from annoy import AnnoyIndex\n",
        "from nltk import edit_distance\n",
        "from urllib.request import urlretrieve\n",
        "from random import random, sample, choice, shuffle\n",
        "from nltk.tokenize import RegexpTokenizer"
      ],
      "metadata": {
        "id": "mVb8x6c3yYrz"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading bigrams and fivegrams for ngram model."
      ],
      "metadata": {
        "id": "acchpbqmxl10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1LauUKpATAXc6rvY5xKhS7rFWoJRxYt93'\n",
        "output = 'bigrams.txt'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "bigrams = {}\n",
        "unigrmas = {}\n",
        "for a in open('./bigrams.txt', 'r', encoding = \"ISO-8859-1\").readlines():\n",
        "    b, c, d = a.strip().split()\n",
        "    bigrams[(c, d)] = int(b)\n",
        "    if tuple([c]) not in unigrmas.keys():\n",
        "        unigrmas[tuple([c])] = int(b)\n",
        "    else:\n",
        "        unigrmas[tuple([c])] += int(b)"
      ],
      "metadata": {
        "id": "Qc-mNdUH-XE8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8de34dcf-1f5e-4d26-e395-1bcc835de2aa"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LauUKpATAXc6rvY5xKhS7rFWoJRxYt93\n",
            "To: /content/bigrams.txt\n",
            "100%|██████████| 17.7M/17.7M [00:00<00:00, 55.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://drive.google.com/uc?id=1N7uOUeP8rPnLfXSoJV4L4PN-R1uh-bd9'\n",
        "output = '5gram.txt'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "fivegram = {}\n",
        "for a in open('./5gram.txt', 'r', encoding = \"ISO-8859-1\").readlines():\n",
        "    b, c, d, e, f, g = a.strip().split()\n",
        "    fivegram[(c, d, e, f, g)] = int(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPjHphVbYQUv",
        "outputId": "b93049eb-5733-4727-8aa4-d96b585c405e"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1N7uOUeP8rPnLfXSoJV4L4PN-R1uh-bd9\n",
            "To: /content/5gram.txt\n",
            "100%|██████████| 28.3M/28.3M [00:00<00:00, 79.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading Norvig unigrams [1] with word frequency and building vocab based on this set."
      ],
      "metadata": {
        "id": "0fqZN7lTxtF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://norvig.com/ngrams/count_1w.txt\"\n",
        "filename = \"norvig.txt\"\n",
        "urlretrieve(url, filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeU1e5AMNRD1",
        "outputId": "1b1833c5-9840-45a5-da16-8f209d9ee9c5"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('norvig.txt', <http.client.HTTPMessage at 0x7c3a5d78b640>)"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORDS = set([a.strip().split()[0]\n",
        "             for a in open(filename, 'r',\n",
        "                           encoding = \"ISO-8859-1\").readlines()][:15000])"
      ],
      "metadata": {
        "id": "Ren-apEEsJd-"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building classes for my custom SpellChecker and Norvig solution"
      ],
      "metadata": {
        "id": "Iwdo6h6Rx5kS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpellChecker:\n",
        "    # Custom spellchecker\n",
        "    def __init__(self, WORDS, ngrams, freq):\n",
        "        \"\"\"\n",
        "        Initializing vocabulars of the spellchecker\n",
        "\n",
        "        :param WORDS: frequent words vocabular\n",
        "        :param ngram: list of ngram frequencies\n",
        "        :param freq: frequencies of word unigrams\n",
        "        \"\"\"\n",
        "        self.WORDS = WORDS\n",
        "        self.letters = string.ascii_lowercase\n",
        "        self.ngrams = ngrams\n",
        "        self.freq = freq\n",
        "\n",
        "        self.dim = len(self.letters) + 1\n",
        "        self.embeddings = np.zeros((len(WORDS), self.dim), dtype=np.float16)\n",
        "        self.text_embedding() # Running embedding of each word in vocab\n",
        "        self.embedding_index = self.get_vector_index() # Get Annoy index\n",
        "\n",
        "        self.tokenizer = RegexpTokenizer(r'\\w+') # Tokenizer\n",
        "\n",
        "    def embed(self, text):\n",
        "        \"\"\"\n",
        "        Word embedding based on symbol frequencies in words\n",
        "\n",
        "        :param text: string word\n",
        "        :return: vector of embedding\n",
        "        \"\"\"\n",
        "        vect = []\n",
        "        base = {i:0 for i in self.letters}\n",
        "\n",
        "        # Add unknown token\n",
        "        base['<UNK>'] = 0\n",
        "        for i in list(text.lower()):\n",
        "            # Count each symbol in words\n",
        "            if i in base.keys():\n",
        "                base[i] += 1\n",
        "            else:\n",
        "                base['<UNK>'] += 1\n",
        "        vect = list(base.values())\n",
        "        return vect\n",
        "\n",
        "    def text_embedding(self):\n",
        "        \"\"\"\n",
        "        Embed all words in vocab\n",
        "        \"\"\"\n",
        "        print(\"Running embedding...\")\n",
        "        for i, item in enumerate(tqdm(self.WORDS)):\n",
        "            # Embed each word of vocab\n",
        "            emb = self.embed(item)\n",
        "            if emb is not None:\n",
        "                self.embeddings[i, :] = emb\n",
        "        print(\"Embeddig finished!\\n\")\n",
        "\n",
        "    def get_vector_index(self):\n",
        "        \"\"\"\n",
        "        Creating Annoy index based on embedded data\n",
        "\n",
        "        :return: Annoy index\n",
        "        \"\"\"\n",
        "        # Use manhattan distance for measuring similarity\n",
        "        idx = AnnoyIndex(self.dim, 'manhattan')\n",
        "        idx.set_seed(33)\n",
        "        print(\"Building an index...\")\n",
        "        for i in tqdm(range(self.embeddings.shape[0])):\n",
        "            idx.add_item(i, self.embeddings[i])\n",
        "        print(\"Building trees...\")\n",
        "        idx.build(37)\n",
        "        idx.save('./my_annoy.ann')\n",
        "        print(\"Finished!\")\n",
        "        return idx\n",
        "\n",
        "    def get_kNN_embeddings(self, embedding, k, index):\n",
        "        \"\"\"\n",
        "        Creating Annoy index based on embedded data\n",
        "\n",
        "        :param embedding: vector of embedded text\n",
        "        :param k: number of neighbors\n",
        "        :param index: Annoy index\n",
        "        :return: ids of neighbors\n",
        "        \"\"\"\n",
        "        return index.get_nns_by_vector(embedding, k)\n",
        "\n",
        "    def get_top_k(self, query, idx_k = 150, idx_top = 70, max_k = 15, min_d = 2):\n",
        "        \"\"\"\n",
        "        Get top of best candidates for replacement\n",
        "\n",
        "        :param query: word with potential mistake\n",
        "        :param idx_k: k neighbors by Annoy\n",
        "        :param idx_top: limit Annoy neighbors\n",
        "        :param max_k: limit edit distance neighbors\n",
        "        :param min_d: minimal acceptable distance\n",
        "        :return: top words\n",
        "        \"\"\"\n",
        "        if len(query) != 0:\n",
        "            # Get indexes of nearest neighbors\n",
        "            idx = self.get_kNN_embeddings(self.embed(query), 150,\n",
        "                                          self.embedding_index)[:70]\n",
        "            d = {}\n",
        "            for i in range(len(idx)):\n",
        "                # Measure distances between word and potential candidates\n",
        "                word = list(self.WORDS)[idx[i]]\n",
        "                d[word] = edit_distance(query, word)\n",
        "            # Get words with shortest edit distance\n",
        "            top_words = [x for x, y in sorted(d.items(), key=lambda x:x[1]) if y <= min_d][:max_k]\n",
        "            return top_words\n",
        "        return ['']\n",
        "\n",
        "    def get_neighbors(self, idx, tokens, n):\n",
        "        \"\"\"\n",
        "        Get ngrams from sentence with specified word\n",
        "\n",
        "        :param idx: id of word in tokens list\n",
        "        :param tokens: list of sentence tokens\n",
        "        :param n: size of ngram\n",
        "        :return: list of ngrams\n",
        "        \"\"\"\n",
        "        neighbors = []\n",
        "        for i in range(idx - n + 1, idx + 1):\n",
        "            if i < 0 or i + n > len(tokens):\n",
        "                continue\n",
        "            else:\n",
        "                part = [x.lower() for x in tokens[i:i+n]]\n",
        "                neighbors.append(part)\n",
        "        if len(neighbors) != 0:\n",
        "            return neighbors\n",
        "        else:\n",
        "            return [[x.lower()] for x in tokens]\n",
        "\n",
        "    def get_probability(self, ngram):\n",
        "        \"\"\"\n",
        "        Get probability of geting such a ngram: P(A|B) = P(A,B)/P(B)\n",
        "\n",
        "        :param ngram: ngram of words\n",
        "        :return: probability of getting word, seen words before\n",
        "        \"\"\"\n",
        "        t = tuple(ngram)\n",
        "        if t in self.ngrams.keys():\n",
        "            if tuple(ngram[:-1]) in self.freq.keys():\n",
        "                # Smoothing\n",
        "                return (int(self.ngrams[t])+1)/(int(self.freq[tuple(ngram[:-1])]) + len(self.freq))\n",
        "        return 1/len(self.freq)\n",
        "\n",
        "    def perplexity(self, probabilities):\n",
        "        \"\"\"\n",
        "        Get perplexity: PP = (1/P_1*...P_i*...*P_n)^(1/n)\n",
        "\n",
        "        :param probabilities: list of probabilities\n",
        "        :return: value of perplexity\n",
        "        \"\"\"\n",
        "        return (1/np.prod(probabilities))**(1/len(probabilities))\n",
        "\n",
        "    def get_probabilities(self, true_word, top_words, neighbors):\n",
        "        \"\"\"\n",
        "        Get perplexities for list of top words\n",
        "\n",
        "        :param true_word: candidate word for replacement\n",
        "        :param top_words:\n",
        "        \"\"\"\n",
        "        perps = []\n",
        "        for word in top_words:\n",
        "            ps = []\n",
        "            for ngram in neighbors:\n",
        "                idx = ngram.index(true_word.lower())\n",
        "                new_ngram = ngram[:idx] + [word] + ngram[idx+1:]\n",
        "                # Searching for probability of such ngram with word replaced by candidate\n",
        "                p = self.get_probability(new_ngram)\n",
        "                ps.append(p)\n",
        "            # Calculate perplexities\n",
        "            perp = self.perplexity(ps)\n",
        "            perps.append(perp)\n",
        "        return perps\n",
        "\n",
        "    def spell_check(self, sentence, n = 2):\n",
        "        \"\"\"\n",
        "        SpellChecking procedure\n",
        "\n",
        "        :param sentence: sentence to spellcheck\n",
        "        :param n: size of ngram\n",
        "        :return: changed sentence\n",
        "        \"\"\"\n",
        "        sent = sentence\n",
        "        # Tokenizing sentence\n",
        "        tokens = self.tokenizer.tokenize(sentence)\n",
        "        for i, word in enumerate(tokens):\n",
        "            if word.lower() in self.WORDS:\n",
        "                # Skip words from vocab\n",
        "                continue\n",
        "            else:\n",
        "                # Get best candidates\n",
        "                top_words = self.get_top_k(word, min_d = 2)\n",
        "                if len(top_words) == 0:\n",
        "                    # The word is unknown\n",
        "                    continue\n",
        "                else:\n",
        "                    # Find best replacement\n",
        "                    best_perplexity = -1\n",
        "                    best_word = \"\"\n",
        "                    neighbors = self.get_neighbors(i, tokens, n)\n",
        "                    probs = self.get_probabilities(word, top_words, neighbors)\n",
        "                    for j, prob in enumerate(probs):\n",
        "                        if prob < best_perplexity or best_perplexity == -1:\n",
        "                            best_word = top_words[j]\n",
        "                            best_perplexity = prob\n",
        "                    sent = sent.replace(word, best_word)\n",
        "                    idx = tokens.index(word)\n",
        "                    tokens = tokens[:idx] + [best_word] + tokens[idx+1:]\n",
        "        return sent\n",
        "sc = SpellChecker(WORDS, bigrams, unigrmas)"
      ],
      "metadata": {
        "id": "YR8k7tX5RBhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3966eae5-2bc0-4ace-8efa-e683e6825a33"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running embedding...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15000/15000 [00:00<00:00, 127353.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddig finished!\n",
            "\n",
            "Building an index...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15000/15000 [00:00<00:00, 216410.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building trees...\n",
            "Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_1 = \"I like dking sport in the morning.\"\n",
        "sentence_2 = \"Only clever dking rules the country.\"\n",
        "\n",
        "print(sc.spell_check(sentence_1))\n",
        "print(sc.spell_check(sentence_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATJW3-2ExvNG",
        "outputId": "68af7e48-ac6d-4c92-8b06-1703c3508e01"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I like doing sport in the morning.\n",
            "Only clever king rules the country.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Norvig solution from [3]\n",
        "class Norvig:\n",
        "    def __init__(self):\n",
        "        url = \"https://norvig.com/big.txt\"\n",
        "        filename = \"big.txt\"\n",
        "        urlretrieve(url, filename)\n",
        "        self.WORDS = Counter(self.words(open('big.txt').read()))\n",
        "\n",
        "    def words(self, text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "    def P(self, word, N = None):\n",
        "        if N is None:\n",
        "            N = sum(self.WORDS.values())\n",
        "        return self.WORDS[word] / N\n",
        "\n",
        "    def correction(self, word):\n",
        "        \"Most probable spelling correction for word.\"\n",
        "        return max(self.candidates(word), key=self.P)\n",
        "\n",
        "    def candidates(self, word):\n",
        "        \"Generate possible spelling corrections for word.\"\n",
        "        return (self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word])\n",
        "\n",
        "    def known(self, words):\n",
        "        \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "        return set(w for w in words if w in self.WORDS)\n",
        "\n",
        "    def edits1(self, word):\n",
        "        \"All edits that are one edit away from `word`.\"\n",
        "        letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "        deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "        inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "        return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "    def edits2(self, word):\n",
        "        \"All edits that are two edits away from `word`.\"\n",
        "        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
        "\n",
        "    def full_correct(self, sentence):\n",
        "        tokenizer = RegexpTokenizer(r'\\w+')\n",
        "        tokens = tokenizer.tokenize(sentence)\n",
        "        for token in tokens:\n",
        "            correct_token = self.correction(token)\n",
        "            sentence = sentence.replace(token, correct_token)\n",
        "        return sentence\n",
        "\n",
        "norvig = Norvig()"
      ],
      "metadata": {
        "id": "ZbnQ3FVBwGV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Justify your decisions"
      ],
      "metadata": {
        "id": "v2JMraJIwyjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following ideas were used to create a custom spellchecker:\n",
        "1. If a word exists in the vocabulary, then it is spelled correctly and does not need corrections;\n",
        "\n",
        "2. If the word does not exist in the vocabulary, there are two possible outcomes:\n",
        "         - The word in the vocabulary that is misspelled;\n",
        "         - A new word that does not exist in the vocabulary at all.\n",
        "\n",
        "3. In order to find out, one can check if it is possible to get a word from the dictionary by a few corrections. This suggests measuring the edit distance between the misspelled word and potential candidates for correction [2]. However, edit distance is a relatively long operation, so computing it over the entire vocabulary might be too long to check a single word. For this reason, I had the idea to use a faster search option - indexing. I chose Annoy as such a tool.\n",
        "\n",
        "4. Annoy [3] (Approximate Nearest Neighbors Oh Yeah) is a C++ library with Python bindings to search for points in space that are close to a given query point. Annoy is using random projections and by building up a tree. At every intermediate node in the tree, a random hyperplane is chosen, which divides the space into two subspaces. This hyperplane is chosen by sampling two points from the subset and taking the hyperplane equidistant from them. We do this k times so that we get a forest of trees. k has to be tuned to your need, by looking at what tradeoff you have between precision and performance. Annoy is almost as fast as the fastest libraries, and it has the ability to use static files as indexes. In particular, this means you can share index across processes. Annoy also decouples creating indexes from loading them, so you can pass around indexes as files and map them into memory quickly. Another nice thing of Annoy is that it tries to minimize memory footprint so the indexes are quite small.\n",
        "\n",
        "5. To build Annoy trees one needs to map words into some n-dimensional space, in other words, to do embedding. I thought it might make sense to encode words through the frequency of the letters they contain. This way each word would correspond to a (26 + 1)-dimensional vector. The +1 is added to keep track of unknown characters (non-letters) that might occur in the words. For such embedding, it may also be reasonable to train Annoy using Manhattan distance.\n",
        "\n",
        "6. Now you can quickly select candidates with a similar set of letters for further analysis. Among the words obtained with Annoy, you can quickly calculate the edit distance between them and the word with a mistake. To narrow down the circle one may sort by edit distance and leave no more than 15 candidates with edit distance not more than 2. If there are no such variants, it means that the word is new, no corrections are needed, it is simply not in the dictionary.\n",
        "\n",
        "7. Now I used ngrams to select the best candidate so that the spellchecker will be context sensitive. To do this, I use a sliding window to go through the sentence with the replaced word and search the dictionary for the probability of encountering such a bigram. Here I also apply smoothing for words that are not in the bigram dictionary. For this purpose, the size of unique words in the dictionary is added to the denominator of the probability, as if there is at least one combination of the given word with all other words, and one is added to the numerator. For completely unfamiliar words, I take a probability equal to 1/\"size of the dictionary\". Then I calculate perplexity for the whole sentence. The sentence with the lowest perplexity had the best candidate for replacement, and I choose exactly it to replace the word with mistake."
      ],
      "metadata": {
        "id": "aQVbKqX8w1qL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on a test set"
      ],
      "metadata": {
        "id": "8kkkAtonwqdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset generation"
      ],
      "metadata": {
        "id": "AtO35Bnb9kMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate the dataset, it was decided to take 5grams as a basis, like correctly written sentences. For each of them two random words were chosen, each of which will be distorted. Two mistakes are made in each of chosen words. For this purpose, a random error code is generated firstly: 1 - deletion of a random letter, 2 - replacement of a random letter, 3 - addition of a random letter. After that, the selected change is made in the word. In this way sentences with errors corresponding to the correct ones are generated."
      ],
      "metadata": {
        "id": "z7i8PcU1mrxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mistakes(sentence):\n",
        "    \"\"\"\n",
        "    Adding distortion to sentence\n",
        "\n",
        "    :param sentence: sentence to be noised\n",
        "    :return: distorted sentence\n",
        "    \"\"\"\n",
        "    tokens = RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
        "    sample_list = sample(tokens, 2)\n",
        "    for sam in sample_list:\n",
        "        for j in range(2):\n",
        "            ch = sample(range(1,4), 1)\n",
        "            if ch == 1:\n",
        "                # random letter remove\n",
        "                sentence = sentence.replace(sam, sam.replace(choice(sam), ''))\n",
        "            elif ch == 2:\n",
        "                # change random letter\n",
        "                sentence = sentence.replace(sam, sam.replace(choice(sam),\n",
        "                                                choice(string.ascii_letters).lower()))\n",
        "            else:\n",
        "                # add random letter\n",
        "                idx = choice(range(len(sam)))\n",
        "                sam_r = sam[:idx] + choice(string.ascii_letters).lower() + sam[idx:]\n",
        "                sentence = sentence.replace(sam, sam_r)\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "UfcitBaGMxE6"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the accuracy of the algorithm, the following was used: Each word from the correct sentence is compared with each word from the corrected sentence and the proportion of correct words was counted. Then the average of this metric over all sentences in the dataset is calculated. Due to the fact that 2 words out of 5 words in a sentence were distorted, the initial value of this metric is 0.6. An output model that gives this metric a value of ~0.6 does not correct errors or does so incorrectly."
      ],
      "metadata": {
        "id": "b-de9EwZopss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check(sent_1, sent_2):\n",
        "    \"\"\"\n",
        "    \"Accuracy\"-like metric for testing sentence\n",
        "\n",
        "    :param sent_1: correct sentence\n",
        "    :param sent_2: corrected sentence\n",
        "    :return: ration of correct words\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    tokens_1 = tokenizer.tokenize(sent_1.lower())\n",
        "    tokens_2 = tokenizer.tokenize(sent_2.lower())\n",
        "    for i, j in zip(tokens_1, tokens_2):\n",
        "        if i == j:\n",
        "            correct += 1\n",
        "    return correct/len(tokens_1)"
      ],
      "metadata": {
        "id": "olS3sG8RNm6g"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate simple 5gram-based dataset\n",
        "simple_dataset = []\n",
        "num = 3000\n",
        "j = 0\n",
        "with open('./5gram.txt', 'r', encoding = \"ISO-8859-1\") as f:\n",
        "    text = f.read().split('\\n')\n",
        "    shuffle(text)\n",
        "    for j in tqdm(range(num if num < len(text) else len(text))):\n",
        "        _, c, d, e, f, g = text[j].split('\\t')\n",
        "        sentence = \" \".join([c, d, e, f, g]).lower()\n",
        "        simple_dataset.append([sentence, mistakes(sentence)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJbV7DnNhmZv",
        "outputId": "f7434c96-0bfc-43e4-d8ee-641847ac6d49"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [00:00<00:00, 17578.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation"
      ],
      "metadata": {
        "id": "ko5L7v4XwSgt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I run dataset on both my and Norvig's spellcheckers [4] and measure time and 'accuracy'."
      ],
      "metadata": {
        "id": "0qNVM1Ixrc-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# My SpellCheker evaluation\n",
        "sc_accuracy = 0\n",
        "for i in tqdm(range(num)):\n",
        "    sent = simple_dataset[i]\n",
        "    correct = sent[0]\n",
        "    mistake = sent[1]\n",
        "    changed = sc.spell_check(mistake)\n",
        "    metric = check(correct, changed)\n",
        "    sc_accuracy += metric\n",
        "sc_accuracy /= num\n",
        "sc_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbBAiBjg3RGn",
        "outputId": "1a8872e8-b08d-48ae-d4eb-b508aa43a341"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 3000/3000 [02:35<00:00, 19.35it/s]\n",
            "0.9238714285714308\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Norvig SpellChecker evaluation\n",
        "norvig_accuracy = 0\n",
        "for i in tqdm(range(num)):\n",
        "    sent = simple_dataset[i]\n",
        "    correct = sent[0]\n",
        "    mistake = sent[1]\n",
        "    changed = norvig.full_correct(mistake)\n",
        "    metric = check(correct, changed)\n",
        "    norvig_accuracy += metric\n",
        "norvig_accuracy /= num\n",
        "norvig_accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6T7Qtcd4jG5U",
        "outputId": "b1e4bf90-cdd4-4e69-856b-989420186f1b"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 3000/3000 [02:32<00:00, 19.74it/s]\n",
            "0.8764269841269858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion:\n",
        "Comparing the result of my spellchecker with Norvig's solution I concluded that my spellchecker is more accurate (0.924) than Norvig's (0.876), although Norvig's spellchecker is slightly faster. This is due to the fact that I have to check the ngram set, while having context sensetive solution. So, in general I can conclude that my spellchecker performed very well!"
      ],
      "metadata": {
        "id": "sLUJB1fGrUua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References:\n",
        "\n",
        "[1] - https://norvig.com/ngrams/\n",
        "\n",
        "[2] - https://github.com/spotify/annoy\n",
        "\n",
        "[3] - https://wisdomml.in/edit-distance-based-spell-corrector-in-nlp/#:~:text=Edit%20distance%20is%20a%20method,similar%20the%20two%20words%20are.\n",
        "\n",
        "[4] - https://norvig.com/spell-correct.html"
      ],
      "metadata": {
        "id": "bSjfku_CPutN"
      }
    }
  ]
}