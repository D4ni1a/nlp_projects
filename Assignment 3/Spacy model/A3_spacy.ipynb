{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP. Assignment 3. Nested Named Entity Recognition\n",
    "---\n",
    "\n",
    "Name: Shulepin Danila\n",
    "\n",
    "Innopolis email: d.shulepin@innopolis.university\n",
    "\n",
    "CodaLab nickname: D4n1la\n",
    "\n",
    "GitHub nickname: D4ni1a\n",
    "\n",
    "GitHub repository: https://github.com/D4ni1a/nlp_projects/tree/main/Assignment%203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a field of natural language processing dedicated to categorizing named entities within textual content. These named entities encompass distinct types of categories. Nested named entity recognition is a subtask of NER that seeks to locate and classify nested named entities (i.e., hierarchically structured entities) mentioned in unstructured text.\n",
    "\n",
    "The significance of NER extends across diverse applications, encompassing information extraction, question answering, chatbots, sentiment analysis, and recommendation systems, underscoring its pivotal role in advancing multiple areas of natural language understanding and utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning SpaCy Model for Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy stands as a leading natural language processing (NLP) library, renowned for its efficiency and versatility in handling various linguistic tasks.\n",
    "\n",
    "This open-source library offers pre-trained models for many tasks, including named entity recognition. SpaCy apart is fast and memory efficient, making it particularly adept at processing large volumes of text in real-time. Moreover, it has a user-friendly interface, extensive language support, integration with deep learning frameworks and its support of a custom training and fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:42:13.534685Z",
     "iopub.status.busy": "2024-04-27T19:42:13.533870Z",
     "iopub.status.idle": "2024-04-27T19:42:26.838145Z",
     "shell.execute_reply": "2024-04-27T19:42:26.837138Z",
     "shell.execute_reply.started": "2024-04-27T19:42:13.534651Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !pip install thinc==8.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:37:10.785240Z",
     "iopub.status.busy": "2024-04-27T19:37:10.784851Z",
     "iopub.status.idle": "2024-04-27T19:37:14.084322Z",
     "shell.execute_reply": "2024-04-27T19:37:14.083442Z",
     "shell.execute_reply.started": "2024-04-27T19:37:10.785203Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dshul\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10vGDK96wji8twLD-2wz6XdG7G_foQSbk\n",
      "To: C:\\Users\\dshul\\Desktop\\NLP\\A3\\best_cat\\dev.jsonl\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 588k/588k [00:00<00:00, 2.22MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NjHU20IgEJ1gZD4eCTmmzHnvjpDG_M5h\n",
      "To: C:\\Users\\dshul\\Desktop\\NLP\\A3\\best_cat\\test.jsonl\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 507k/507k [00:00<00:00, 1.77MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Wy0TjYjIUcN6q9pUTZ96CMICDrTodjyy\n",
      "To: C:\\Users\\dshul\\Desktop\\NLP\\A3\\best_cat\\train.jsonl\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4.87M/4.87M [00:01<00:00, 4.11MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train.jsonl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# I uploaded the dataset into my Google Drive\n",
    "# At first step, download data via gdown\n",
    "url = 'https://drive.google.com/uc?id=10vGDK96wji8twLD-2wz6XdG7G_foQSbk'\n",
    "output = 'dev.jsonl'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1NjHU20IgEJ1gZD4eCTmmzHnvjpDG_M5h'\n",
    "output = 'test.jsonl'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1Wy0TjYjIUcN6q9pUTZ96CMICDrTodjyy'\n",
    "output = 'train.jsonl'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:37:14.086011Z",
     "iopub.status.busy": "2024-04-27T19:37:14.085552Z",
     "iopub.status.idle": "2024-04-27T19:37:14.090041Z",
     "shell.execute_reply": "2024-04-27T19:37:14.089243Z",
     "shell.execute_reply.started": "2024-04-27T19:37:14.085984Z"
    }
   },
   "outputs": [],
   "source": [
    "train_file = \"./train.jsonl\"\n",
    "test_file = \"./test.jsonl\"\n",
    "dev_file = \"./dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:37:14.092318Z",
     "iopub.status.busy": "2024-04-27T19:37:14.092053Z",
     "iopub.status.idle": "2024-04-27T19:37:14.165186Z",
     "shell.execute_reply": "2024-04-27T19:37:14.164512Z",
     "shell.execute_reply.started": "2024-04-27T19:37:14.092296Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read dataset from file in JSON format\n",
    "train = [json.loads(line) for line in open(train_file, 'r')]\n",
    "test = [json.loads(line) for line in open(test_file, 'r')]\n",
    "val = [json.loads(line) for line in open(dev_file, 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NEREL dataset contains sentences with the following labels: AGE, AWARD, CITY, COUNTRY, CRIME, DATE, DISEASE, EVENT, FACILITY, FAMILY, IDEOLOGY, LANGUAGE, LAW, LOCATION, MONEY, NATIONALITY, NUMBER, ORDINAL, ORGANIZATION, PENALTY, PERCENT, PERSON, PRODUCT, PROFESSION, RELEGION, STATE_OR_PROV, TIME, WORK_OF_ART, ORGANIZATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:37:14.166377Z",
     "iopub.status.busy": "2024-04-27T19:37:14.166140Z",
     "iopub.status.idle": "2024-04-27T19:37:14.172372Z",
     "shell.execute_reply": "2024-04-27T19:37:14.171497Z",
     "shell.execute_reply.started": "2024-04-27T19:37:14.166356Z"
    }
   },
   "outputs": [],
   "source": [
    "ner_list = [\"AGE\", \"AWARD\", \"CITY\", \"COUNTRY\", \"CRIME\", \"DATE\", \"DISEASE\",\n",
    "            \"DISTRICT\", \"EVENT\", \"FACILITY\", \"FAMILY\", \"IDEOLOGY\", \"LANGUAGE\",\n",
    "            \"LAW\", \"LOCATION\", \"MONEY\", \"NATIONALITY\", \"NUMBER\", \"ORDINAL\",\n",
    "            \"ORGANIZATION\", \"PENALTY\", \"PERCENT\", \"PERSON\", \"PRODUCT\",\n",
    "            \"PROFESSION\", \"RELIGION\", \"STATE_OR_PROVINCE\", \"TIME\", \"WORK_OF_ART\"\n",
    "            ]\n",
    "ner_to_num = {word: str(i+1) for i, word in enumerate(ner_list)}\n",
    "num_to_ner = {str(i+1): word for i, word in enumerate(ner_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:37:14.174043Z",
     "iopub.status.busy": "2024-04-27T19:37:14.173709Z",
     "iopub.status.idle": "2024-04-27T19:37:14.194637Z",
     "shell.execute_reply": "2024-04-27T19:37:14.193928Z",
     "shell.execute_reply.started": "2024-04-27T19:37:14.174014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset frequencies:\n",
      "AGE - 657\n",
      "AWARD - 404\n",
      "CITY - 1261\n",
      "COUNTRY - 2510\n",
      "CRIME - 221\n",
      "DATE - 2689\n",
      "DISEASE - 220\n",
      "DISTRICT - 103\n",
      "EVENT - 3335\n",
      "FACILITY - 424\n",
      "FAMILY - 24\n",
      "IDEOLOGY - 273\n",
      "LANGUAGE - 54\n",
      "LAW - 405\n",
      "LOCATION - 314\n",
      "MONEY - 179\n",
      "NATIONALITY - 437\n",
      "NUMBER - 1107\n",
      "ORDINAL - 614\n",
      "ORGANIZATION - 4088\n",
      "PENALTY - 92\n",
      "PERCENT - 68\n",
      "PERSON - 5119\n",
      "PRODUCT - 245\n",
      "PROFESSION - 5039\n",
      "RELIGION - 89\n",
      "STATE_OR_PROVINCE - 412\n",
      "TIME - 182\n",
      "WORK_OF_ART - 270\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate frequency of each named entity in the train dataset\n",
    "count = {num_to_ner[str(i+1)]:0 for i in range(len(ner_list))}\n",
    "for j in range(len(train)):\n",
    "    for i in train[j]['ners']:\n",
    "        a, b, c = i\n",
    "        count[c] += 1\n",
    "\n",
    "print(\"Train dataset frequencies:\")\n",
    "for key, value in count.items():\n",
    "    print(f\"{key} - {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the list of frequencies, the labels are really imbalanced. Only around 15 of them occur quit frequent: PERSON, PROFESSION, ORGANIZATION, EVENT, DATE, COUNTRY, CITY, NUMBER, AGE, ORDINAL, NATIONALITY, FACILITY, STATE_OR_PROVINCE, LAW, AWARD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num = 15\n",
    "num = 29\n",
    "acceptance_list = [a for a, b in sorted(count.items(), key=lambda x:-x[1])[:num]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used dataset should have specific format in order to train Spacy models on it. In Spacy model end index of the substring should be equal to real end index + 1. Therefore, one should increment end indexes of the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:37:14.195836Z",
     "iopub.status.busy": "2024-04-27T19:37:14.195543Z",
     "iopub.status.idle": "2024-04-27T19:37:14.231287Z",
     "shell.execute_reply": "2024-04-27T19:37:14.230440Z",
     "shell.execute_reply.started": "2024-04-27T19:37:14.195786Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://ubiai.tools/fine-tuning-spacy-models-customizing-named-entity-recognition/\n",
    "\n",
    "def convert(data):\n",
    "    \"\"\"\n",
    "    Converting data into the format accepted by Spacy\n",
    "    \n",
    "    :param data: dictinary-like dataset\n",
    "    :return: dictinary-like dataset accepted by the Spacy model\n",
    "    \"\"\"\n",
    "    data_spacy = {'classes' : [str(i) for i in range(len(ner_list))], 'annotations' : []}\n",
    "    for i in range(len(data)):\n",
    "        ners = data[i]['ners']\n",
    "        tmp = {}\n",
    "        # Extract sentence\n",
    "        tmp['text'] = data[i]['sentences']\n",
    "        tmp['entities'] = []\n",
    "        for j in range(len(ners)):\n",
    "            start, end, label = ners[j]\n",
    "            # Append annotations\n",
    "            if label in acceptance_list:\n",
    "                new_label = ner_to_num[label]\n",
    "                # Incremental end\n",
    "                tmp['entities'].append((start, end + 1, new_label))\n",
    "        if len(tmp['entities']) != 0:\n",
    "            data_spacy['annotations'].append(tmp)\n",
    "    return data_spacy\n",
    "\n",
    "train_data_spacy = convert(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:38:04.389490Z",
     "iopub.status.busy": "2024-04-27T19:38:04.388613Z",
     "iopub.status.idle": "2024-04-27T19:38:04.585842Z",
     "shell.execute_reply": "2024-04-27T19:38:04.585059Z",
     "shell.execute_reply.started": "2024-04-27T19:38:04.389455Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create DocBin object to hold serialized annotations\n",
    "nlp_doc = spacy.blank(\"ru\")\n",
    "doc_bin = DocBin()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For building the Spacy model was used SpanCategorizer. The SpanCategorizer is a Spacy component used to structure annotation for a wide variety of labeled spans, including  overlapping annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "# Building doc for SpanCategorizer\n",
    "for training_example in tqdm(train_data_spacy['annotations']):\n",
    "    text = training_example['text']\n",
    "    labels = training_example['entities']\n",
    "    doc = nlp_doc.make_doc(text)\n",
    "    ents = []\n",
    "    # Building spans groups\n",
    "    for start, end, label in labels:\n",
    "        span = doc.char_span(start, end, label=label, alignment_mode=\"contract\")\n",
    "        if span is None:\n",
    "            continue\n",
    "        else:\n",
    "            ents.append(span)\n",
    "    # Alternative to SpanCategorizer\n",
    "    # filtered_ents = filter_spans(ents)\n",
    "    # doc.ents = filtered_ents\n",
    "    doc.spans[\"sc\"] = ents\n",
    "    doc_bin.add(doc)\n",
    "\n",
    "doc_bin.to_disk(\"training_data.spacy\") # save the docbin object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:38:11.256495Z",
     "iopub.status.busy": "2024-04-27T19:38:11.255784Z",
     "iopub.status.idle": "2024-04-27T19:38:11.263146Z",
     "shell.execute_reply": "2024-04-27T19:38:11.262165Z",
     "shell.execute_reply.started": "2024-04-27T19:38:11.256458Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://spacy.io/usage/training\n",
    "# Building basic configuration file\n",
    "\n",
    "base_config = '''[paths]\n",
    "train = \"./training_data.spacy\"\n",
    "dev = \"./training_data.spacy\"\n",
    "vectors = null\n",
    "[system]\n",
    "gpu_allocator = null\n",
    "\n",
    "[nlp]\n",
    "lang = \"ru\"\n",
    "pipeline = [\"tok2vec\",\"spancat\"]\n",
    "batch_size = 1000\n",
    "\n",
    "[components]\n",
    "\n",
    "[components.tok2vec]\n",
    "factory = \"tok2vec\"\n",
    "\n",
    "[components.tok2vec.model]\n",
    "@architectures = \"spacy.Tok2Vec.v2\"\n",
    "\n",
    "[components.tok2vec.model.embed]\n",
    "@architectures = \"spacy.MultiHashEmbed.v2\"\n",
    "width = ${components.tok2vec.model.encode.width}\n",
    "attrs = [\"NORM\", \"PREFIX\", \"SUFFIX\", \"SHAPE\"]\n",
    "rows = [5000, 1000, 2500, 2500]\n",
    "include_static_vectors = false\n",
    "\n",
    "[components.tok2vec.model.encode]\n",
    "@architectures = \"spacy.MaxoutWindowEncoder.v2\"\n",
    "width = 96\n",
    "depth = 4\n",
    "window_size = 1\n",
    "maxout_pieces = 3\n",
    "\n",
    "[components.spancat]\n",
    "factory = \"spancat\"\n",
    "max_positive = null\n",
    "scorer = {\"@scorers\":\"spacy.spancat_scorer.v1\"}\n",
    "spans_key = \"sc\"\n",
    "threshold = 0.5\n",
    "\n",
    "[components.spancat.model]\n",
    "@architectures = \"spacy.SpanCategorizer.v1\"\n",
    "\n",
    "[components.spancat.model.reducer]\n",
    "@layers = \"spacy.mean_max_reducer.v1\"\n",
    "hidden_size = 128\n",
    "\n",
    "[components.spancat.model.scorer]\n",
    "@layers = \"spacy.LinearLogistic.v1\"\n",
    "nO = null\n",
    "nI = null\n",
    "\n",
    "[components.spancat.model.tok2vec]\n",
    "@architectures = \"spacy.Tok2VecListener.v1\"\n",
    "width = ${components.tok2vec.model.encode.width}\n",
    "\n",
    "[components.spancat.suggester]\n",
    "@misc = \"spacy.ngram_suggester.v1\"\n",
    "sizes = [1,2,3]\n",
    "\n",
    "[corpora]\n",
    "\n",
    "[corpora.train]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.train}\n",
    "max_length = 0\n",
    "\n",
    "[corpora.dev]\n",
    "@readers = \"spacy.Corpus.v1\"\n",
    "path = ${paths.dev}\n",
    "max_length = 0\n",
    "\n",
    "[training]\n",
    "dev_corpus = \"corpora.dev\"\n",
    "train_corpus = \"corpora.train\"\n",
    "\n",
    "[training.optimizer]\n",
    "@optimizers = \"Adam.v1\"\n",
    "\n",
    "[training.batcher]\n",
    "@batchers = \"spacy.batch_by_words.v1\"\n",
    "discard_oversize = false\n",
    "tolerance = 0.2\n",
    "\n",
    "[training.batcher.size]\n",
    "@schedules = \"compounding.v1\"\n",
    "start = 100\n",
    "stop = 1000\n",
    "compound = 1.001\n",
    "\n",
    "[initialize]\n",
    "vectors = ${paths.vectors}'''\n",
    "\n",
    "with open(\"base_config.cfg\", \"w\") as f:\n",
    "    f.write(base_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T19:42:26.840442Z",
     "iopub.status.busy": "2024-04-27T19:42:26.840133Z",
     "iopub.status.idle": "2024-04-27T19:54:33.719183Z",
     "shell.execute_reply": "2024-04-27T19:54:33.718023Z",
     "shell.execute_reply.started": "2024-04-27T19:42:26.840413Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Auto-filled config with all values\u001b[0m\n",
      "\u001b[38;5;2m✔ Saved config\u001b[0m\n",
      "config.cfg\n",
      "You can now add your data and train your pipeline:\n",
      "python -m spacy train config.cfg --paths.train ./train.spacy --paths.dev ./dev.spacy\n",
      "\u001b[38;5;4mℹ Saving to output directory: .\u001b[0m\n",
      "\u001b[38;5;4mℹ Using GPU: 0\u001b[0m\n",
      "\u001b[1m\n",
      "=========================== Initializing pipeline ===========================\u001b[0m\n",
      "[2024-04-27 19:42:37,732] [INFO] Set up nlp object from config\n",
      "[2024-04-27 19:42:37,773] [INFO] Pipeline: ['tok2vec', 'spancat']\n",
      "[2024-04-27 19:42:37,779] [INFO] Created vocabulary\n",
      "[2024-04-27 19:42:37,779] [INFO] Finished initializing nlp object\n",
      "[2024-04-27 19:42:50,927] [INFO] Initialized pipeline components: ['tok2vec', 'spancat']\n",
      "\u001b[38;5;2m✔ Initialized pipeline\u001b[0m\n",
      "\u001b[1m\n",
      "============================= Training pipeline =============================\u001b[0m\n",
      "\u001b[38;5;4mℹ Pipeline: ['tok2vec', 'spancat']\u001b[0m\n",
      "\u001b[38;5;4mℹ Initial learn rate: 0.001\u001b[0m\n",
      "E    #       LOSS TOK2VEC  LOSS SPANCAT  SPANS_SC_F  SPANS_SC_P  SPANS_SC_R  SCORE \n",
      "---  ------  ------------  ------------  ----------  ----------  ----------  ------\n",
      "  0       0        312.99       7599.03        0.45        0.23       39.32    0.00\n",
      "  0     200        764.75      31836.75       33.12       71.20       21.58    0.33\n",
      "  0     400          8.05       7947.96       38.86       84.65       25.21    0.39\n",
      "  1     600         11.29       7387.25       52.31       90.90       36.72    0.52\n",
      "  1     800         12.57       6412.50       57.02       89.19       41.91    0.57\n",
      "  1    1000         11.62       6277.55       59.97       93.45       44.15    0.60\n",
      "  2    1200         11.48       5672.41       63.92       91.86       49.01    0.64\n",
      "  2    1400         11.20       5202.37       66.57       88.78       53.24    0.67\n",
      "  3    1600         11.75       5130.85       67.73       93.06       53.24    0.68\n",
      "  3    1800         10.20       4581.74       68.85       93.46       54.50    0.69\n",
      "  3    2000         11.10       4638.34       70.33       91.47       57.12    0.70\n",
      "  4    2200         11.24       4636.79       71.53       90.91       58.95    0.72\n",
      "  4    2400         11.51       4395.35       72.39       92.98       59.26    0.72\n",
      "  5    2600         12.27       4196.06       73.35       94.39       59.98    0.73\n",
      "  5    2800         10.37       3810.31       73.88       93.33       61.14    0.74\n",
      "  5    3000         12.52       4163.00       74.86       93.88       62.25    0.75\n",
      "  6    3200         11.09       3644.01       75.60       94.51       63.00    0.76\n",
      "  6    3400         18.67       3688.05       75.56       94.09       63.13    0.76\n",
      "  6    3600         12.43       3750.87       76.00       95.13       63.27    0.76\n",
      "  7    3800          9.79       3483.75       76.38       94.82       63.95    0.76\n",
      "  7    4000         12.51       3619.50       76.91       95.75       64.26    0.77\n",
      "  8    4200         12.24       3485.10       76.98       96.48       64.04    0.77\n",
      "  8    4400         12.61       3548.49       77.41       94.36       65.62    0.77\n",
      "  8    4600         12.62       3255.59       78.10       95.80       65.92    0.78\n",
      "  9    4800         11.51       3144.45       78.61       96.34       66.39    0.79\n",
      "  9    5000         12.67       3249.23       78.49       94.95       66.89    0.78\n",
      "\u001b[38;5;2m✔ Saved pipeline to output directory\u001b[0m\n",
      "model-last\n"
     ]
    }
   ],
   "source": [
    "# Initializing configuration file and training model on max of 5000 steps\n",
    "!python -m spacy init fill-config base_config.cfg config.cfg\n",
    "!python -m spacy train config.cfg --output ./ --training.max_steps 5000 --paths.train ./training_data.spacy --paths.dev ./training_data.spacy --gpu-id 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of the NER-spans for the test set on the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-27T20:37:42.663472Z",
     "iopub.status.busy": "2024-04-27T20:37:42.662723Z",
     "iopub.status.idle": "2024-04-27T20:37:44.995081Z",
     "shell.execute_reply": "2024-04-27T20:37:44.994126Z",
     "shell.execute_reply.started": "2024-04-27T20:37:42.663440Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:01<00:00, 35.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loading best model\n",
    "nlp_ner = spacy.load(\"./model-best\")\n",
    "\n",
    "output = []\n",
    "for i in tqdm(range(len(test))):\n",
    "    item = test[i]\n",
    "    tmp = {}\n",
    "    tmp[\"id\"] = item['id']\n",
    "    sentence = item['senences']\n",
    "    # Predicting NER-spans for test set\n",
    "    ner_doc = nlp_ner(sentence)\n",
    "    span = ner_doc.spans['sc']\n",
    "    ners = []\n",
    "    for j in range(len(span)):\n",
    "        label = span[j].label_\n",
    "        start = span[j].start_char\n",
    "        end = span[j].end_char - 1\n",
    "        ners.append([start, end, str(label)])\n",
    "    tmp[\"ners\"] = ners\n",
    "    output.append(tmp)\n",
    "\n",
    "# Saving the results\n",
    "# !mkdir ./output/\n",
    "with open('./output/test.jsonl', 'w') as f:\n",
    "    for i in range(len(output)):\n",
    "        f.write(f'{output[i]}\\n')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
