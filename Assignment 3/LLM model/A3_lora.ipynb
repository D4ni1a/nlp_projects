{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP. Assignment 3. Nested Named Entity Recognition\n",
    "---\n",
    "\n",
    "Name: Shulepin Danila\n",
    "\n",
    "Innopolis email: d.shulepin@innopolis.university\n",
    "\n",
    "CodaLab nickname: D4n1la\n",
    "\n",
    "GitHub nickname: D4ni1a\n",
    "\n",
    "GitHub repository: https://github.com/D4ni1a/nlp_projects/tree/main/Assignment%203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a field of natural language processing dedicated to categorizing named entities within textual content. These named entities encompass distinct types of categories. Nested named entity recognition is a subtask of NER that seeks to locate and classify nested named entities (i.e., hierarchically structured entities) mentioned in unstructured text.\n",
    "\n",
    "The significance of NER extends across diverse applications, encompassing information extraction, question answering, chatbots, sentiment analysis, and recommendation systems, underscoring its pivotal role in advancing multiple areas of natural language understanding and utilization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning LLMs with LORA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large language model (LLM) is a type of language model that stands out for its capacity to generate texts for a wide range of contexts and carry out different NLP tasks. Text generation is primarily powered by LLMs. To put it briefly, they are made up of large pretrained transformer models that have been trained to predict the next token based on input text.\n",
    "\n",
    "A method called Parameter Efficient Fine Tuning (PEFT) was created to optimize models with the least amount of finances and resources. When it comes to domain-specific activities that require model modification, PEFT is an excellent option. By using PEFT, one may successfully adapt the pre-trained model to a specific task with fewer parameters while preserving important knowledge from it. It is possible to achieve parameter efficient fine-tuning in a variety of methods. LoRA, or Low Rank Parameter, is one of the most popular and useful.\n",
    "\n",
    "The goal of LoRA is to minimize the number of trainable parameters with low-rank representations. Low-rank matrices that are updated and learned are created by decomposing the weight matrix. The pretrained model's parameters are all kept fixed. After training, the low-rank matrices come back to their original weights. Consequently, a LoRA model has far fewer parameters, making it easier to store and train.\n",
    "\n",
    "LoRA: https://arxiv.org/pdf/2106.09685"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T14:15:18.308139Z",
     "iopub.status.busy": "2024-04-28T14:15:18.307293Z",
     "iopub.status.idle": "2024-04-28T14:15:55.405820Z",
     "shell.execute_reply": "2024-04-28T14:15:55.404733Z",
     "shell.execute_reply.started": "2024-04-28T14:15:18.308106Z"
    },
    "id": "u1P9Nlz8Sixw",
    "outputId": "63c0ebda-6bd2-45af-beda-cb8dfc54bfe0"
   },
   "outputs": [],
   "source": [
    "# !pip install gdown\n",
    "# !pip install -q peft transformers datasets evaluate seqeval accelerate bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T14:15:55.407915Z",
     "iopub.status.busy": "2024-04-28T14:15:55.407612Z",
     "iopub.status.idle": "2024-04-28T14:16:15.664215Z",
     "shell.execute_reply": "2024-04-28T14:16:15.663462Z",
     "shell.execute_reply.started": "2024-04-28T14:15:55.407885Z"
    },
    "id": "MaF9VZaJTvPE",
    "outputId": "02032b13-d4a7-4972-f0aa-fca3c4222ac5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 14:16:06.818950: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-28 14:16:06.819044: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-28 14:16:06.972879: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gdown\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T14:16:15.665874Z",
     "iopub.status.busy": "2024-04-28T14:16:15.665271Z",
     "iopub.status.idle": "2024-04-28T14:16:18.907239Z",
     "shell.execute_reply": "2024-04-28T14:16:18.906362Z",
     "shell.execute_reply.started": "2024-04-28T14:16:15.665847Z"
    },
    "id": "ahjE3zotSoRz",
    "outputId": "7cf8c005-11a4-48d7-9205-53dfc8a6c13e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=10vGDK96wji8twLD-2wz6XdG7G_foQSbk\n",
      "To: /kaggle/working/dev.jsonl\n",
      "100%|██████████| 588k/588k [00:00<00:00, 123MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1NjHU20IgEJ1gZD4eCTmmzHnvjpDG_M5h\n",
      "To: /kaggle/working/test.jsonl\n",
      "100%|██████████| 507k/507k [00:00<00:00, 116MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Wy0TjYjIUcN6q9pUTZ96CMICDrTodjyy\n",
      "To: /kaggle/working/train.jsonl\n",
      "100%|██████████| 4.87M/4.87M [00:00<00:00, 260MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'train.jsonl'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "\n",
    "# I uploaded the dataset into my Google Drive\n",
    "# At first step, download data via gdown\n",
    "url = 'https://drive.google.com/uc?id=10vGDK96wji8twLD-2wz6XdG7G_foQSbk'\n",
    "output = 'dev.jsonl'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1NjHU20IgEJ1gZD4eCTmmzHnvjpDG_M5h'\n",
    "output = 'test.jsonl'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1Wy0TjYjIUcN6q9pUTZ96CMICDrTodjyy'\n",
    "output = 'train.jsonl'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T14:16:18.909712Z",
     "iopub.status.busy": "2024-04-28T14:16:18.909401Z",
     "iopub.status.idle": "2024-04-28T14:16:18.913809Z",
     "shell.execute_reply": "2024-04-28T14:16:18.912892Z",
     "shell.execute_reply.started": "2024-04-28T14:16:18.909688Z"
    },
    "id": "KD6b2QVjSo0n"
   },
   "outputs": [],
   "source": [
    "train_file = \"./train.jsonl\"\n",
    "test_file = \"./test.jsonl\"\n",
    "dev_file = \"./dev.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T14:16:18.915268Z",
     "iopub.status.busy": "2024-04-28T14:16:18.915006Z",
     "iopub.status.idle": "2024-04-28T14:16:20.152553Z",
     "shell.execute_reply": "2024-04-28T14:16:20.151755Z",
     "shell.execute_reply.started": "2024-04-28T14:16:18.915246Z"
    },
    "id": "R0E7RE-JSqRM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read dataset from file in JSON format\n",
    "train = [json.loads(line) for line in open(train_file, 'r')]\n",
    "test = [json.loads(line) for line in open(test_file, 'r')]\n",
    "val = [json.loads(line) for line in open(dev_file, 'r')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NEREL dataset contains sentences with the following labels: AGE, AWARD, CITY, COUNTRY, CRIME, DATE, DISEASE, EVENT, FACILITY, FAMILY, IDEOLOGY, LANGUAGE, LAW, LOCATION, MONEY, NATIONALITY, NUMBER, ORDINAL, ORGANIZATION, PENALTY, PERCENT, PERSON, PRODUCT, PROFESSION, RELEGION, STATE_OR_PROV, TIME, WORK_OF_ART, ORGANIZATION."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T14:16:20.153958Z",
     "iopub.status.busy": "2024-04-28T14:16:20.153678Z",
     "iopub.status.idle": "2024-04-28T14:16:20.160277Z",
     "shell.execute_reply": "2024-04-28T14:16:20.159363Z",
     "shell.execute_reply.started": "2024-04-28T14:16:20.153934Z"
    },
    "id": "IGz-89Y7SrTG"
   },
   "outputs": [],
   "source": [
    "ner_list = [\"AGE\", \"AWARD\", \"CITY\", \"COUNTRY\", \"CRIME\", \"DATE\", \"DISEASE\",\n",
    "            \"DISTRICT\", \"EVENT\", \"FACILITY\", \"FAMILY\", \"IDEOLOGY\", \"LANGUAGE\",\n",
    "            \"LAW\", \"LOCATION\", \"MONEY\", \"NATIONALITY\", \"NUMBER\", \"ORDINAL\",\n",
    "            \"ORGANIZATION\", \"PENALTY\", \"PERCENT\", \"PERSON\", \"PRODUCT\",\n",
    "            \"PROFESSION\", \"RELIGION\", \"STATE_OR_PROVINCE\", \"TIME\", \"WORK_OF_ART\"\n",
    "            ]\n",
    "ner_to_num = {word: str(i+1) for i, word in enumerate(ner_list)}\n",
    "num_to_ner = {str(i+1): word for i, word in enumerate(ner_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T14:16:20.161807Z",
     "iopub.status.busy": "2024-04-28T14:16:20.161487Z",
     "iopub.status.idle": "2024-04-28T14:16:20.173302Z",
     "shell.execute_reply": "2024-04-28T14:16:20.172473Z",
     "shell.execute_reply.started": "2024-04-28T14:16:20.161776Z"
    },
    "id": "FkJROi8ISxUA"
   },
   "outputs": [],
   "source": [
    "# Choose device to train on\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:50:39.481714Z",
     "iopub.status.busy": "2024-04-28T15:50:39.480806Z",
     "iopub.status.idle": "2024-04-28T15:50:39.488880Z",
     "shell.execute_reply": "2024-04-28T15:50:39.487790Z",
     "shell.execute_reply.started": "2024-04-28T15:50:39.481678Z"
    },
    "id": "2ocpGQT8mZbl"
   },
   "outputs": [],
   "source": [
    "def generate_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generating text prompt from a single data point of a train data\n",
    "    \n",
    "    :param data_point: single data point of a training dataset\n",
    "    :return: text prompt\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "            Act as an expert analyst with more than 20 years of experience in linguistics. \n",
    "            You need to extract nested named entities from the following text:\n",
    "            \"{data_point[\"sentences\"]}\" and write it at Output.\n",
    "\n",
    "            Output = {data_point[\"ners\"]}.\n",
    "            \"\"\".strip()\n",
    "\n",
    "\n",
    "def generate_test_prompt(data_point):\n",
    "    \"\"\"\n",
    "    Generating text prompt from a single data point of a test data\n",
    "    \n",
    "    :param data_point: single data point of a test dataset\n",
    "    :return: text prompt\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "            Act as an expert analyst with more than 20 years of experience in linguistics. \n",
    "            You need to extract nested named entities from the following text:\n",
    "            \"{data_point[\"senences\"]}\" and write it at Output.\n",
    "\n",
    "            Output = .\n",
    "            \"\"\".strip()\n",
    "\n",
    "def train_prompter(x):\n",
    "    \"\"\"\n",
    "    Converting a single data point of a test dataset\n",
    "    \n",
    "    :param x: single data point of a test dataset\n",
    "    :return: text prompt\n",
    "    \"\"\"\n",
    "    text, ners = x['sentences'], x['ners']\n",
    "    tmp = []\n",
    "    for i in range(len(ners)):\n",
    "        # Replace boundries by a word\n",
    "        a, b, c = ners[i]\n",
    "        tmp.append([text[a:b+1], c])\n",
    "    return generate_prompt({'sentences':text, 'ners':tmp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T14:16:20.187073Z",
     "iopub.status.busy": "2024-04-28T14:16:20.186811Z",
     "iopub.status.idle": "2024-04-28T14:16:20.246258Z",
     "shell.execute_reply": "2024-04-28T14:16:20.245447Z",
     "shell.execute_reply.started": "2024-04-28T14:16:20.187051Z"
    },
    "id": "bEmIXjRhmIQD"
   },
   "outputs": [],
   "source": [
    "# Convert train and test data\n",
    "X_train = pd.DataFrame([train_prompter(x) for x in train], columns=[\"text\"])\n",
    "train_data = Dataset.from_pandas(X_train)\n",
    "X_test = pd.DataFrame([generate_test_prompt(x) for x in test], columns=[\"text\"])\n",
    "test_data = Dataset.from_pandas(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a BitsAndBytesConfig object and loading Llama2 LLM with quantization config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T14:16:20.248883Z",
     "iopub.status.busy": "2024-04-28T14:16:20.248625Z",
     "iopub.status.idle": "2024-04-28T14:18:53.359306Z",
     "shell.execute_reply": "2024-04-28T14:18:53.358474Z",
     "shell.execute_reply.started": "2024-04-28T14:16:20.248861Z"
    },
    "id": "YwBPkpCIXtKR",
    "outputId": "57510a7c-53ef-484e-abc8-505fc4275464"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42d0eaf95f14323a3e33c4de20eda74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model path from Kaggle, as this notebook was run on Kaggle\n",
    "# https://www.kaggle.com/datasets/lizhecheng/llama2-7b-hf\n",
    "model_name = \"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\"\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # the model weights\n",
    "    bnb_4bit_quant_type=\"nf4\", # quantization type\n",
    "    bnb_4bit_compute_dtype=compute_dtype, # the float16 data type\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=device,\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T14:18:53.360748Z",
     "iopub.status.busy": "2024-04-28T14:18:53.360449Z",
     "iopub.status.idle": "2024-04-28T14:18:56.398775Z",
     "shell.execute_reply": "2024-04-28T14:18:56.397804Z",
     "shell.execute_reply.started": "2024-04-28T14:18:53.360722Z"
    },
    "id": "MUqHiwHUodJQ"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac05d757473748659af4631e71b5ebfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c02665d66da45c3b6dba6b77d9ee830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/129 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_dir = \"trained_weigths\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,  # directory to save and repository id\n",
    "    num_train_epochs=2,  # number of training epochs\n",
    "    per_device_train_batch_size=1,  # batch size\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,  # gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,  # log every 25 steps\n",
    "    learning_rate=2e-4,  # learning rate\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,  # max gradient norm\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,  # warmup ratio\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",  # cosine learning rate scheduler\n",
    "    report_to=\"tensorboard\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Build trainer for Llama LLM model\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=train_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=1024,\n",
    "    packing=False,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,\n",
    "        \"append_concat_token\": False,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T14:18:56.400654Z",
     "iopub.status.busy": "2024-04-28T14:18:56.399974Z",
     "iopub.status.idle": "2024-04-28T15:41:13.319302Z",
     "shell.execute_reply": "2024-04-28T15:41:13.318482Z",
     "shell.execute_reply.started": "2024-04-28T14:18:56.400618Z"
    },
    "id": "zc-IRw19v38R"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 1:18:13, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.195784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.137321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=16, training_loss=1.2363789081573486, metrics={'train_runtime': 4936.4174, 'train_samples_per_second': 0.052, 'train_steps_per_second': 0.003, 'total_flos': 1.0362271594364928e+16, 'train_loss': 1.2363789081573486, 'epoch': 1.97})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:41:13.320832Z",
     "iopub.status.busy": "2024-04-28T15:41:13.320531Z",
     "iopub.status.idle": "2024-04-28T15:41:14.664605Z",
     "shell.execute_reply": "2024-04-28T15:41:14.663741Z",
     "shell.execute_reply.started": "2024-04-28T15:41:13.320806Z"
    },
    "id": "c1Zcdctov5wC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('trained_weigths/tokenizer_config.json',\n",
       " 'trained_weigths/special_tokens_map.json',\n",
       " 'trained_weigths/tokenizer.model',\n",
       " 'trained_weigths/added_tokens.json',\n",
       " 'trained_weigths/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving weights\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:41:23.253559Z",
     "iopub.status.busy": "2024-04-28T15:41:23.252514Z",
     "iopub.status.idle": "2024-04-28T15:42:43.481271Z",
     "shell.execute_reply": "2024-04-28T15:42:43.480273Z",
     "shell.execute_reply.started": "2024-04-28T15:41:23.253525Z"
    }
   },
   "outputs": [],
   "source": [
    "# delete and call garbage collector to free memory\n",
    "import gc\n",
    "\n",
    "del [\n",
    "    model,\n",
    "    tokenizer,\n",
    "    peft_config,\n",
    "    trainer,\n",
    "    train_data,\n",
    "    bnb_config,\n",
    "    training_arguments,\n",
    "]\n",
    "del [X_train]\n",
    "del [TrainingArguments, SFTTrainer, LoraConfig, BitsAndBytesConfig]\n",
    "\n",
    "# empty cuda cache several times\n",
    "for _ in range(300):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading model and saved weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T15:43:02.575611Z",
     "iopub.status.busy": "2024-04-28T15:43:02.575239Z",
     "iopub.status.idle": "2024-04-28T15:44:02.434938Z",
     "shell.execute_reply": "2024-04-28T15:44:02.434038Z",
     "shell.execute_reply.started": "2024-04-28T15:43:02.575581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f93611fbdfa42eb8f02cc3a649a1af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./merged_model/tokenizer_config.json',\n",
       " './merged_model/special_tokens_map.json',\n",
       " './merged_model/tokenizer.model',\n",
       " './merged_model/added_tokens.json',\n",
       " './merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "finetuned_model = \"./trained_weigths/\"\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/llama2-7b-hf/Llama2-7b-hf\")\n",
    "\n",
    "# Load model\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    finetuned_model,\n",
    "    torch_dtype=compute_dtype,\n",
    "    return_dict=False,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "merged_model = model.merge_and_unload()\n",
    "merged_model.save_pretrained(\n",
    "    \"./merged_model\", safe_serialization=True, max_shard_size=\"2GB\"\n",
    ")\n",
    "tokenizer.save_pretrained(\"./merged_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of the NER-spans for the test set on the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T17:01:36.838151Z",
     "iopub.status.busy": "2024-04-28T17:01:36.837778Z",
     "iopub.status.idle": "2024-04-28T17:01:36.847154Z",
     "shell.execute_reply": "2024-04-28T17:01:36.846167Z",
     "shell.execute_reply.started": "2024-04-28T17:01:36.838121Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import re\n",
    "\n",
    "def predict(test, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Predicting the test results\n",
    "    \n",
    "    :param test: test data\n",
    "    :param model: trained model\n",
    "    :param tokenizer: tokenizer for model\n",
    "    :return: list of dictionaries with spans\n",
    "    \"\"\"\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(test))):\n",
    "        idx = test[i]['id']\n",
    "        prompt = generate_test_prompt(test[i])\n",
    "        text = test[i][\"senences\"]\n",
    "        pipe = pipeline(\n",
    "            task=\"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=1,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        result = pipe(prompt)\n",
    "        # Predict result and get part after 'Output = '\n",
    "        answer = literal_eval(result[0][\"generated_text\"].split('Output = ')[-1][:-1])\n",
    "        answer_ner = []\n",
    "        # Replace word by its indexes in text\n",
    "        for out in output:\n",
    "            word, label = out[0], out[1]\n",
    "            rgx = re.compile(word)\n",
    "            match = re.search(rgx, text)\n",
    "            answer_ner.append([match.start(), match.end() - 1, label])\n",
    "        out = {'id': idx, 'ners':answer_ner}\n",
    "        y_pred.append(out)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T17:01:28.775708Z",
     "iopub.status.busy": "2024-04-28T17:01:28.775267Z",
     "iopub.status.idle": "2024-04-28T17:01:28.784636Z",
     "shell.execute_reply": "2024-04-28T17:01:28.783628Z",
     "shell.execute_reply.started": "2024-04-28T17:01:28.775661Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:00<00:00, 418143.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Predict NER-spans\n",
    "y_pred = predict(test, merged_model, tokenizer)\n",
    "\n",
    "# Save results\n",
    "# !mkdir ./output/\n",
    "with open('./output/test.jsonl', 'w') as f:\n",
    "    for i in range(len(y_pred)):\n",
    "        f.write(f'{y_pred[i]}\\n')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 3601853,
     "sourceId": 6266221,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
